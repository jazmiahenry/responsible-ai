{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Fun With Word Embeddings\n",
        "\n",
        "GLoVe embeddings can be accessed using on this webpage: https://nlp.stanford.edu/projects/glove/. Here, you can download publicly licensed pre-trained GLoVe word embeddings. There are four zip files of word embeddings available for you to download:\n",
        "\n",
        "    1. Wikipedia 2014 + Gigaword 5 (6B tokens, 400K vocab, uncased, 50d, 100d, 200d, & 300d vectors, 822 MB download)\n",
        "    2. Common Crawl (42B tokens, 1.9M vocab, uncased, 300d vectors, 1.75 GB download)\n",
        "    3. Common Crawl (840B tokens, 2.2M vocab, cased, 300d vectors, 2.03 GB download)\n",
        "    4. Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased, 25d, 50d, 100d, & 200d vectors, 1.42 GB download)\n",
        "\n",
        "We will be using the **Common Crawl 42B tokens** download in this tutorial. However, you can practice using any of the zip files on the GLoVe website."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "d6daac9e-be10-4f14-9dac-f414e260ed4c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Download Data File\n",
        "\n",
        "To begin visualizing GLoVe word embeddings, you must first download the Common Crawl 42B tokens zip file here: https://nlp.stanford.edu/projects/glove/. The download may take a few minutes as the file is 1.75 GB large. \n",
        "\n",
        "Once the file has been downloaded, you will upload the file into your notebook using Pandas. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "c9f177d5-440a-45e6-aa9c-3a04f6c9c03e"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas\n",
        "!pip install numpy"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "25ba50f1-9368-4fb5-b631-93349c8cce18"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd \n",
        "import numpy as np "
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1690491641869
        }
      },
      "id": "fd4ed7e1-303c-44e6-b846-2006b1b7251c"
    },
    {
      "cell_type": "code",
      "source": [
        "#import words from txt file and save as a dictionary\n",
        "\n",
        "word_embs = {}\n",
        "\n",
        "#change the file name to reflect your txt file's path\n",
        "with open(\"glove.42B.300d.txt\", 'r') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        words = values[0]\n",
        "        word_embs[words]=np.asarray(values[1:], \"float32\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1690491853376
        }
      },
      "id": "a8da5142-a70f-4a2d-8777-268513637438"
    },
    {
      "cell_type": "code",
      "source": [
        "word_embs"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1690491859138
        }
      },
      "id": "0e8ef671-ca3f-4bc8-bb64-46e92f8a1d65"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Similarity Using Euclidean Distance\n",
        "\n",
        "Euclidean distance is a measure of the length of a line connecting two points in Euclidean space. For GLoVe embeddings, the similarity of two words is measured by their Euclidean distance. Words that are closer together in Euclidean space are considered more similar and words that are farther apart are considered dissimilar. \n",
        "\n",
        "The **scipy** package in Python gives you the ability to measure Euclidean distance between multiple points in a 1D array. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "e267b61c-e2a5-4259-8a78-abbccaaee375"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scipy"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": true
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "9351473c-1d65-42f4-8e8e-ac6d73fc238a"
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy.spatial import distance\n",
        "\n",
        "\n",
        "\n",
        "def get_distances(X, Y):\n",
        "    dist = distance.euclidean(X, Y)\n",
        "    return dist"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1690492973432
        }
      },
      "id": "5fa63ffd-9e7c-400f-aeb0-373255b3ce67"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The Euclidean distance between man and queen is \", get_distances(word_embs[\"man\"], word_embs[\"queen\"]))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1690493286925
        }
      },
      "id": "bf5ba2b1-d235-46ff-9ae7-be95208614f8"
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"The Euclidean distance between woman and king is \",get_distances(word_embs[\"woman\"], word_embs[\"king\"]))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1690493322904
        }
      },
      "id": "0e8a9044-3a98-487d-8e7e-7501e0e5eee3"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Words that have a higher euclidean distance score are words that are considered to be more dissimilar. One thing we notice is that man and queen and woman and king are dissimilar words. However, if you are to run the above code to run man and king or woman and queen, the distance between the two words will be smaller indicating that man and king is more similar than woman and king. \n",
        "\n",
        "While this is a good first step in your analysis, however, it is important to note that king and queen have lower distance scores when compared together and punctuation points have a closer distance to the words king and queen than the words man and woman. This is because in the dataset, punctuation marks occur most frequently with these words. To get a more reliable distance score, we would need to perform some more analysis. \n",
        "\n",
        "Let's first start by looking at a list of the words most similar to queen and king by creating a **most_similar** function that can return to us the most similar words by their distance scores."
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "79b800ff-9439-4959-9b7d-0de3f245719a"
    },
    {
      "cell_type": "code",
      "source": [
        "def most_similar(embs):\n",
        "    return sorted(word_embs.keys(), key=lambda word: distance.euclidean(word_embs[words], embs))"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "874024a3-a093-4031-ace6-b3fa4e31e976"
    },
    {
      "cell_type": "code",
      "source": [
        "#returns the top 5 most similar words by their distance scores. \n",
        "most_similar(word_embs[\"queen\"])[:5]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1690493891103
        }
      },
      "id": "2e449a2e-db87-4210-950b-6c052283a463"
    },
    {
      "cell_type": "code",
      "source": [
        "#returns the top 5 most similar words by their distance scores. \n",
        "most_similar(word_embs[\"king\"])[:5]"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1690493978954
        }
      },
      "id": "228bf869-95cb-483c-85d4-cef8493a2970"
    },
    {
      "cell_type": "markdown",
      "source": [
        "As you can see, punctuation marks and articles are the top 5 most similar words to the word queen and king! This is a very common occurrence when using word embeddings as articles and punctuation marks are used more commonly than words!  "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "2de95248-eaae-4f30-9f93-706eeff3501f"
    },
    {
      "cell_type": "code",
      "source": [
        "print(most_similar(word_embs['king'] - word_embs['man'] + word_embs['woman'])[:5])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1690494611885
        }
      },
      "id": "9136677d-7a11-4479-905f-77a5284146ef"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sentence Preprocessing\n",
        "In Natural Language Processing, you can use a technique called \"removing stop words\" to take away commonly used words in the English language that does not contribute to our understanding of a word's context. This includes things like articles. \n",
        "\n",
        "We will also do things like removing punctuation marks to make sure our analysis is more concise. The **NLTK** package has a way for us to remove stopwords and the **regex** package has a way for us to remove punctuation. We will dowload both packages after we turn our dictionary word embeddings into a list for preprocessing. "
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "d5481153-0475-49c4-9685-8fae1280d898"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nltk\n",
        "!pip install regex"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "87991041-1b20-46f0-9915-04804e530193"
    },
    {
      "cell_type": "code",
      "source": [
        "# turn dictionary into a 2D list for us to perform analysis\n",
        "import string\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "2bb176b7-f118-47a8-9728-46329677bc0d"
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words = stopwords.words('english')\n",
        "punct = '''!\"#$%&'()*+, -./:;<=>?@[\\]^_`{|}~'''\n",
        "stop_words.extend(punct)\n",
        "\n",
        "stop_words"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1690503817371
        }
      },
      "id": "0c0669db-883c-41ef-b348-19587c5c8c6e"
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_dict(dict):\n",
        "    for words in stop_words:  \n",
        "        dict.pop(words, None)\n",
        "    return dict"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1690503847013
        }
      },
      "id": "a7b8a58f-3628-4729-a61a-f8d8091bb607"
    },
    {
      "cell_type": "code",
      "source": [
        "word_embs_clean = clean_dict(word_embs)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1690503875972
        }
      },
      "id": "c4b2c7d9-bd11-4fc1-989a-02fe2f1c0599"
    },
    {
      "cell_type": "code",
      "source": [
        "word_embs_clean"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "gather": {
          "logged": 1690503890997
        }
      },
      "id": "5d4487ca-a011-44f5-8db6-39187dc02f54"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's Think\n",
        "\n",
        "Now that we have removed stop words from our word embeddings dictionary, how does the dictionary look?\n",
        "\n",
        "Do you notice more words or punctuation marks that may be worth removing to improve the cleanliness of the dictionary even further?\n",
        "\n",
        "Run the **most_similar** and **get_distances** functions on the new dictionary. Are the distance scores the same or different? Why may that be? Are the top 5 most similar words the same or different? Why is this?"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "3ffc014e-c264-43c2-b39e-83a32736f412"
    }
  ],
  "metadata": {
    "save_output": true,
    "microsoft": {
      "language": "python",
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "trident": {
          "lakehouse": {
            "known_lakehouses": "[{\"id\":\"16c73b28-fb00-46d7-8428-1bac28f4e57d\"}]",
            "default_lakehouse": "16c73b28-fb00-46d7-8428-1bac28f4e57d"
          }
        }
      }
    },
    "notebook_environment": {},
    "widgets": {},
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "trident": {
      "lakehouse": {
        "default_lakehouse": "16c73b28-fb00-46d7-8428-1bac28f4e57d",
        "known_lakehouses": [
          {
            "id": "16c73b28-fb00-46d7-8428-1bac28f4e57d"
          }
        ],
        "default_lakehouse_name": "responsible",
        "default_lakehouse_workspace_id": "324d0a02-d83c-4342-869c-80be1c670414"
      }
    },
    "spark_compute": {
      "compute_id": "/trident/default",
      "session_options": {
        "enableDebugMode": false,
        "conf": {}
      }
    },
    "language_info": {
      "name": "python",
      "version": "3.8.5",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "synapse_widget": {
      "version": "0.1",
      "state": {}
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}